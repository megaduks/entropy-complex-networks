{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Discovery-of-pivotal-data-instances-using-vertex-energy-and-data-similarity-graph\" data-toc-modified-id=\"Discovery-of-pivotal-data-instances-using-vertex-energy-and-data-similarity-graph-1\">Discovery of pivotal data instances using vertex energy and data similarity graph</a></span><ul class=\"toc-item\"><li><span><a href=\"#Graph-energy\" data-toc-modified-id=\"Graph-energy-1.1\">Graph energy</a></span></li><li><span><a href=\"#Randić-energy\" data-toc-modified-id=\"Randić-energy-1.2\">Randić energy</a></span></li><li><span><a href=\"#Laplacian-energy\" data-toc-modified-id=\"Laplacian-energy-1.3\">Laplacian energy</a></span></li><li><span><a href=\"#Matrix-energies-for-various-topologies-of-small-egocentric-networks\" data-toc-modified-id=\"Matrix-energies-for-various-topologies-of-small-egocentric-networks-1.4\">Matrix energies for various topologies of small egocentric networks</a></span></li></ul></li><li><span><a href=\"#Fine-tuning-of-data-for-unsupervised-clustering\" data-toc-modified-id=\"Fine-tuning-of-data-for-unsupervised-clustering-2\">Fine-tuning of data for unsupervised clustering</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T06:13:26.245589Z",
     "start_time": "2019-10-03T06:13:26.239171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T06:13:27.982507Z",
     "start_time": "2019-10-03T06:13:26.438866Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"...\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from ggplot import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy, scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.preprocessing\n",
    "import logging\n",
    "import requests\n",
    "import itertools\n",
    "\n",
    "import network_energy as ne\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from data2graph.measures import measure, caterogical, numerical\n",
    "from data2graph.measures.measure import Measure\n",
    "from data2graph.network.network import Network\n",
    "from data2graph.network.weight import Weight\n",
    "from data2graph.network import network, load, weight, algorithm\n",
    "\n",
    "from data2graph.datasets import loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "# Discovery of pivotal data instances using vertex energy and data similarity graph\n",
    "\n",
    "In this experiment we verify the usefulness of the concept of graph energy in discovering pivotal data points.\n",
    "We transform relational dataset into graph representation where each data instance is represented by a single vertex\n",
    "and an edge exists between vertices if the similarity between data instances exceeds a given threshold.\n",
    "\n",
    "We use this data similarity graph to estimate the relative importance of data instances. An instance is important\n",
    "if its characterized by high betweenness in data similarity graph. Unfortunately, exact computation of betweenness \n",
    "for large graphs is prohibitively expensive. We present a simple method which allows to estimate the betweenness\n",
    "of vertices with high precision using a novel concept of vertex energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph energy\n",
    "\n",
    "Graph energy of a graph is defined as $E_G(G) = \\sum\\limits_{i=1}^n |\\mu_i|$, where $\\mu_1, \\ldots, \\mu_n$ are the eigenvalues of the adjacency matrix $M_A$ (also known as the *spectrum* of the graph).\n",
    "\n",
    "## Randić energy\n",
    "\n",
    "Randić matrix of the graph $G=\\left<V, E\\right>$ is defined as:\n",
    "\n",
    "$$\n",
    "M_R(i,j)=\n",
    "\\begin{cases}\n",
    "0 & \\mathit{if} & i=j\\\\\n",
    "\\frac{1}{\\sqrt{d_i d_j}} & \\mathit{if} & (i,j) \\in E\\\\\n",
    "0 & \\mathit{if} & (i,j) \\notin E\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Randić energy of a graph is defined as $E_R(G) = \\sum\\limits_{i=1}^n |\\rho_i|$, where $\\rho_1, \\ldots, \\rho_n$ are the eigenvalues of the Randić matrix $M_R$.\n",
    "\n",
    "## Laplacian energy\n",
    "\n",
    "Laplacian matrix of the graph $G=\\left<V, E\\right>$ is defined as:\n",
    "\n",
    "$$\n",
    "M_L(i,j)=\n",
    "\\begin{cases}\n",
    "d_i & \\mathit{if} & i=j\\\\\n",
    "-1 & \\mathit{if} & (i,j) \\in E\\\\\n",
    "0 & \\mathit{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Laplacian energy of a graph is defined as $E_L(G) =  \\sum\\limits_{i=1}^n |\\lambda_i - \\frac{2m}{n}|$, where $\\lambda_1, \\ldots, \\lambda_n$ are the eigenvalues of the Laplacian matrix $M_L$, $n$ is the number of vertices and $m$ is the number of edges in the graph $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T20:53:20.320618Z",
     "start_time": "2019-09-27T20:53:20.304704Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"\n",
    "    Divide a list of vertices `lst` into chunks consisting of `n` vertices\n",
    "    \n",
    "    Tests:\n",
    "    >>> list(chunks([1,2,3,4,5,6], 2))\n",
    "    [(1, 2), (3, 4), (5, 6)]\n",
    "\n",
    "    >>> list(chunks([1,2,3,4,5,6], 4))\n",
    "    [(1, 2, 3, 4), (5, 6)]\n",
    "\n",
    "    >>> list(chunks([], 2))\n",
    "    []\n",
    "\n",
    "    \"\"\"\n",
    "    _lst = iter(lst)\n",
    "    while 1:\n",
    "        x = tuple(itertools.islice(_lst, n))\n",
    "        if not x:\n",
    "            return\n",
    "        yield x\n",
    "        \n",
    "def normalize_df_column(df_column):\n",
    "    \"\"\"\n",
    "    Normalize a dataframe column to the range [0,1]\n",
    "    \n",
    "    Tests:\n",
    "    >>> normalize_df_column(pd.Series([1,2,3,4,5]))\n",
    "    array([[0.  ],\n",
    "           [0.25],\n",
    "           [0.5 ],\n",
    "           [0.75],\n",
    "           [1.  ]])\n",
    "    \"\"\"\n",
    "    x = df_column.values.astype(float)\n",
    "    min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x.reshape(-1,1))\n",
    "    \n",
    "    return x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T20:53:21.056944Z",
     "start_time": "2019-09-27T20:53:21.018723Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix energies for various topologies of small egocentric networks\n",
    "\n",
    "Firstly, let us examine the relationship between the topology of a small egocentric network and its energies. We generate five different egocentric networks representing possible small scale configurations and compute all three types of matrix energies. The results are somehow surprising, graph energy tends to correlate with the degree of connectivity of the egocentric network, Randic energy remains practically constant, and Laplacian energy behaves unpredictably, receiving the maximum value for a custom topology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:28:28.113410Z",
     "start_time": "2019-09-23T20:28:27.455878Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "g_custom = nx.star_graph(n=5)\n",
    "g_custom.add_edge(1,2)\n",
    "g_custom.add_edge(4,5)\n",
    "\n",
    "graphs = [\n",
    "    {'name': 'path', 'graph': nx.path_graph(n=3)},\n",
    "    {'name': 'star', 'graph': nx.star_graph(n=5)},\n",
    "    {'name': 'custom', 'graph': g_custom},\n",
    "    {'name': 'wheel', 'graph': nx.wheel_graph(n=5)},\n",
    "    {'name': 'complete', 'graph': nx.complete_graph(n=5)}\n",
    "]\n",
    "\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "sns.set(rc={'figure.figsize': (15, 4)})\n",
    "fig, ax = plt.subplots(5, 1)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'name': [ g['name'] for g in graphs],\n",
    "     'graph energy': [ne.get_graph_energy(g['graph']) for g in graphs],\n",
    "     'randic energy': [ne.get_randic_energy(g['graph']) for g in graphs],\n",
    "     'laplacian energy': [ne.get_laplacian_energy(g['graph']) for g in graphs]\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.subplot(151)\n",
    "nx.draw(graphs[0]['graph'], node_color=['black','white','black'], edgecolors='black')\n",
    "plt.title(graphs[0]['name'])\n",
    "\n",
    "plt.subplot(152)\n",
    "nx.draw(graphs[1]['graph'], node_color=['white','black', 'black', 'black', 'black', 'black'], edgecolors='black')\n",
    "plt.title(graphs[1]['name'])\n",
    "\n",
    "plt.subplot(153)\n",
    "nx.draw(graphs[2]['graph'], node_color=['white','black','black', 'black', 'black', 'black'], edgecolors='black')\n",
    "plt.title(graphs[2]['name'])\n",
    "\n",
    "plt.subplot(154)\n",
    "nx.draw(graphs[3]['graph'], node_color=['white','black','black', 'black', 'black'], edgecolors='black')\n",
    "plt.title(graphs[3]['name'])\n",
    "\n",
    "plt.subplot(155)\n",
    "nx.draw(graphs[4]['graph'], node_color=['white','black','black', 'black', 'black'], edgecolors='black')\n",
    "plt.title(graphs[4]['name'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(df[['name', 'graph energy', 'randic energy', 'laplacian energy']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start with a star configuration of an egocentric network consisting of an ego and additional $n$ vertices, \n",
    "and we gradually add all remaining edges, until we form a full $K_5$ graph. \n",
    "For each intermediate graph we compute all its energies. We can clearly see that each of matrix energies \n",
    "is measuring a different \"aspect\" of the egocentric network:\n",
    "\n",
    "* randic energy is maximized for topologies very close to the original star-like structure and diminishes as more and more edges are added to the egocentric network\n",
    "* laplacian energy strongly resembles the entropy of adjacency matrix, being maximized half-way between the star structure and the clique structure of the egocentric network\n",
    "* graph energy steadily grows as the density of the egocentric network increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:28:30.193390Z",
     "start_time": "2019-09-23T20:28:29.273880Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from random import shuffle\n",
    "\n",
    "g = nx.star_graph(n=25)\n",
    "\n",
    "results = []\n",
    "\n",
    "edges = list(combinations(range(1, len(g.nodes)), r=2))\n",
    "\n",
    "# comment if you want to add edges in an ordered way\n",
    "shuffle(edges)\n",
    "\n",
    "for (idx, (i, j)) in enumerate(edges):\n",
    "    results.append((idx, ne.get_graph_energy(g), ne.get_randic_energy(g),\n",
    "                    ne.get_laplacian_energy(g)))\n",
    "\n",
    "    g.add_edge(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:28:31.886252Z",
     "start_time": "2019-09-23T20:28:31.878432Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=results,\n",
    "    columns=[\n",
    "        'complexity', 'graph energy', 'randic energy', 'laplacian energy'\n",
    "    ])\n",
    "\n",
    "dfn = df[['graph energy','randic energy','laplacian energy']].apply(lambda s: s/s.max(), axis=0)\n",
    "dfn['complexity'] = df['complexity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:28:32.920553Z",
     "start_time": "2019-09-23T20:28:32.332794Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dfm = pd.melt(\n",
    "    dfn,\n",
    "    value_vars=['graph energy', 'randic energy', 'laplacian energy'],\n",
    "    id_vars='complexity')\n",
    "\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.lineplot(data=dfm, x='complexity', y='value', style='variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning of data for unsupervised clustering\n",
    "\n",
    "In this experiment we create data similarity graphs for a few well-known datasets. \n",
    "For each graph we collect detailed statistics on every vertex:\n",
    "\n",
    "- its betweenness,\n",
    "- its Randić energy,\n",
    "- its Laplacian energy,\n",
    "- and its graph energy.\n",
    "\n",
    "We normalize these variables using MinMax scaling to the range of [0-1]. \n",
    "\n",
    "Finally, we compute the correlation between vertex betweenness and vertex energies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T06:13:33.840751Z",
     "start_time": "2019-10-03T06:13:33.836143Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'inflammation': loader.load_diagnosis_inflammation,\n",
    "    'diagnosis_nephritis': loader.load_diagnosis_nephritis,\n",
    "    'iris': loader.load_iris,\n",
    "    'titanic': loader.load_titanic,\n",
    "    'lenses': loader.load_lenses,\n",
    "    'mushrooms': loader.load_mushrooms,\n",
    "    'breast_cancer': loader.load_breast_cancer_short,\n",
    "    'wine_quality': loader.load_wine_quality_classification,\n",
    "    'pima_diabetes': loader.load_pima_diabetes,\n",
    "    'internet_ads': loader.load_internet_ads_pca,\n",
    "    'housing_prices': loader.load_housing_prices_short,\n",
    "    'ionosphere': loader.load_ionosphere,\n",
    "    'monks1': loader.load_monks_1,\n",
    "    'monks2': loader.load_monks_2,\n",
    "    'monks3': loader.load_monks_3,\n",
    "    'yeast': loader.load_yeast,\n",
    "    'heart_statlog': loader.load_heart_statlog,\n",
    "    'haberman': loader.load_haberman,\n",
    "    'hepatitis': loader.load_hepatitis,\n",
    "    'dermatology': loader.load_dermatology,\n",
    "    'glass': loader.load_glass,\n",
    "    'ecoli': loader.load_ecoli,\n",
    "    'cmc': loader.load_cmc,\n",
    "    'zoo': loader.load_zoo,\n",
    "    'balance_scale': loader.load_balance_scale,\n",
    "    'segmentation': loader.load_segmentation,\n",
    "    'car': loader.load_car,\n",
    "    'house_voting': loader.load_house_voting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T06:13:38.804148Z",
     "start_time": "2019-10-03T06:13:35.142871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic: 1043 rows\n",
      "mushrooms: 8124 rows\n",
      "breast_cancer: 569 rows\n",
      "wine_quality: 1599 rows\n",
      "pima_diabetes: 768 rows\n",
      "internet_ads: 2359 rows\n",
      "housing_prices: 8059 rows\n",
      "ionosphere: 351 rows\n",
      "monks1: 556 rows\n",
      "monks2: 601 rows\n",
      "monks3: 554 rows\n",
      "yeast: 1484 rows\n",
      "heart_statlog: 303 rows\n",
      "haberman: 306 rows\n",
      "dermatology: 358 rows\n",
      "ecoli: 336 rows\n",
      "cmc: 1473 rows\n",
      "balance_scale: 625 rows\n",
      "car: 1728 rows\n",
      "house_voting: 435 rows\n"
     ]
    }
   ],
   "source": [
    "for d in datasets:\n",
    "    X, y, types = datasets[d]()\n",
    "    \n",
    "    if len(X) > 250:\n",
    "        print(f'{d}: {len(X)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T06:14:12.499812Z",
     "start_time": "2019-10-03T06:14:12.465997Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def _compute_difference_clustering_scores(X, vals, n_clusters, score_function, top_k=10):\n",
    "    \n",
    "    model_full = KMeans(n_clusters=n_clusters).fit(X)\n",
    "\n",
    "    original_score = score_function(X, model_full.labels_)\n",
    "    \n",
    "    if score_function == 'Calinski-Harabasz index':\n",
    "        original_score /= len(X)\n",
    "\n",
    "    top_order_idx = [ \n",
    "        idx\n",
    "        for idx, val\n",
    "        in sorted(vals, key=lambda x: x[1], reverse=True)[top_k:]\n",
    "    ]\n",
    "\n",
    "    X_lim = X[top_order_idx]\n",
    "\n",
    "    model_reduced = KMeans(n_clusters=n_clusters).fit(X_lim)\n",
    "\n",
    "    reduced_score = score_function(X_lim, model_reduced.labels_)\n",
    "    \n",
    "    if score_function == 'Calinski-Harabasz index':\n",
    "        reduced_score /= len(X_lim)\n",
    "    \n",
    "    delta_score = (reduced_score - original_score) / original_score\n",
    "    \n",
    "    model_applied = model_reduced.predict(X)\n",
    "    \n",
    "    applied_score = score_function(X, model_applied)\n",
    "    \n",
    "    if score_function == 'Calinski-Harabasz index':\n",
    "        applied_score /= len(X)\n",
    "\n",
    "    applied_delta_score = (applied_score - original_score) / original_score\n",
    "    \n",
    "    # check if results hold in comparison with random order as well\n",
    "    random_order_idx = [n for n in G.nodes]\n",
    "    np.random.shuffle(random_order_idx)\n",
    "    X_rand = X[random_order_idx[top_k:]]\n",
    "    \n",
    "    model_random = KMeans(n_clusters=n_clusters).fit(X_rand)\n",
    "\n",
    "    random_score = score_function(X_rand, model_random.labels_)\n",
    "    \n",
    "    if score_function == 'Calinski-Harabasz index':\n",
    "        random_score /= len(X)\n",
    "\n",
    "    random_delta_score = (reduced_score - random_score) / random_score\n",
    "    \n",
    "    return delta_score, applied_delta_score, random_delta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T06:14:14.098831Z",
     "start_time": "2019-10-03T06:14:14.094883Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def _find_optimum_number_clusters(X, scoring_function, n_values=10):\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    k_min = 2\n",
    "    k_max = np.sqrt(len(X)).astype(int)\n",
    "    \n",
    "    for k in np.linspace(k_min, k_max, n_values).astype(int).tolist():\n",
    "        \n",
    "        model = KMeans(n_clusters=k).fit(X)\n",
    "        score = scoring_function(X, model.labels_)\n",
    "        \n",
    "        scores.append((k, score))\n",
    "        \n",
    "    if scoring_function == 'Davies-Bouldin index':\n",
    "        return min(scores, key=itemgetter(1))[0] # get the index of the smallest value\n",
    "    else:\n",
    "        return max(scores, key=itemgetter(1))[0] # get the index of the largest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-03T07:01:09.050Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b9a0fc017e493697655fe0b957a33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "\n",
    "results = pd.DataFrame(columns=[\n",
    "    'dataset',\n",
    "    'order',\n",
    "    'nodes',\n",
    "    'betweenness',\n",
    "    'randic_energy',\n",
    "    'graph_energy',\n",
    "    'score_name',\n",
    "    'score_diff',\n",
    "    'score_improve_applied',\n",
    "    'score_improve_over_random',\n",
    "    'top_k'\n",
    "])\n",
    "\n",
    "# TODO: repeat experiment for another size of neighborhood\n",
    "radius = 1\n",
    "\n",
    "# simple function to find quantiles\n",
    "thresholds = lambda x: [ int(x*p) for p in np.arange(0.00, 0.11, 0.01)]\n",
    "\n",
    "for dataset in tqdm(datasets):\n",
    "\n",
    "    X, y, types = datasets[dataset]()\n",
    "\n",
    "    if len(X) > 2000:\n",
    "        continue\n",
    "    if len(X) < 250:\n",
    "        continue\n",
    "        \n",
    "    _thresholds = thresholds(len(X))\n",
    "\n",
    "    measure_strategy = measure.Measure(numerical_strategy=numerical.mahalanobis,\n",
    "                                       categorical_strategy=caterogical.goodall_3)\n",
    "\n",
    "    measures = measure_strategy.compute(X, types)\n",
    "\n",
    "    network_strategy = network.Network(load_strategy=partial(load.load_graph_weight_similarity, beta=0.15),\n",
    "                                       weight_strategy=algorithm.weight_by_degree)\n",
    "\n",
    "    G = network_strategy.load(measures, y)\n",
    "\n",
    "    be_order = nx.betweenness_centrality(G, k=None, normalized=True)\n",
    "    re_order = ne.randic_centrality(G, radius=radius, normalized=True)\n",
    "    ge_order = ne.graph_energy_centrality(G, radius=radius, normalized=True)\n",
    "    le_order = ne.laplacian_centrality(G, radius=radius, normalized=True)\n",
    "\n",
    "    orders = {\n",
    "        'betweenness': be_order,\n",
    "        'randic': re_order,\n",
    "        'graph': ge_order,\n",
    "        'laplacian': le_order,\n",
    "    }\n",
    "    \n",
    "    score_functions = {\n",
    "        'Silhouette score': silhouette_score,\n",
    "        'Calinski-Harabasz index': calinski_harabasz_score,\n",
    "        'Davies-Bouldin index': davies_bouldin_score\n",
    "    }\n",
    "\n",
    "    for score_function in score_functions:\n",
    "        \n",
    "        n_clusters = _find_optimum_number_clusters(X, scoring_function=score_functions[score_function], n_values=5)\n",
    "        \n",
    "        for order in orders:\n",
    "\n",
    "            for top_k in _thresholds:\n",
    "\n",
    "                score_diff, score_applied_diff, score_random_diff = _compute_difference_clustering_scores(\n",
    "                    X, orders[order].items(), n_clusters=n_clusters, score_function=score_functions[score_function], top_k=top_k)\n",
    "                \n",
    "                # davis-bouldin values are the other way around\n",
    "                if score_function == 'Davies-Bouldin index':\n",
    "                    score_diff *= -1\n",
    "                    score_random_diff *= -1\n",
    "                    score_random_diff *= -1\n",
    "\n",
    "                _dict = {\n",
    "                    'dataset': dataset,\n",
    "                    'order': order,\n",
    "                    'nodes': list(G.nodes),\n",
    "                    'betweenness': list(be_order.values()),\n",
    "                    'randic_energy': list(re_order.values()),\n",
    "                    'graph_energy': list(ge_order.values()),\n",
    "                    'laplacian_energy': list(le_order.values()),\n",
    "                    'score_function': score_function,\n",
    "                    'score_diff': score_diff,\n",
    "                    'score_improve_applied': score_applied_diff,\n",
    "                    'score_improve_over_random': score_random_diff,\n",
    "                    'top_k': top_k,\n",
    "                }\n",
    "\n",
    "                _result = pd.DataFrame.from_dict(_dict)\n",
    "\n",
    "                results = pd.concat([results, _result], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T06:35:41.549473Z",
     "start_time": "2019-10-02T06:35:39.870419Z"
    }
   },
   "outputs": [],
   "source": [
    "df_plot = results.groupby(['order','top_k', 'score_function'])[[\n",
    "    'score_diff',\n",
    "    'score_improve_applied',\n",
    "    'score_improve_over_random'\n",
    "]].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T06:35:41.656914Z",
     "start_time": "2019-10-02T06:35:41.555073Z"
    }
   },
   "outputs": [],
   "source": [
    "df_plot.groupby(['score_function','order']).agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T06:36:08.081244Z",
     "start_time": "2019-10-02T06:35:49.762736Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for score_function in score_functions:\n",
    "    \n",
    "    for y in ['score_diff', 'score_improve_applied', 'score_improve_over_random']:\n",
    "\n",
    "        plt.figure()\n",
    "        \n",
    "        g = sns.barplot(\n",
    "            data=df_plot[df_plot.score_function == score_function], \n",
    "            x='order',\n",
    "            y=y\n",
    "        ).set_title(f'{score_function} {y}')\n",
    "\n",
    "        g\n",
    "\n",
    "        g = sns.barplot(\n",
    "            data=df_plot[df_plot.score_function == score_function], \n",
    "            x='order',\n",
    "            y=y\n",
    "        ).set_title(f'{score_function} {y}')\n",
    "\n",
    "        g\n",
    "\n",
    "        g = sns.barplot(\n",
    "            data=df_plot[df_plot.score_function == score_function], \n",
    "            x='order',\n",
    "            y=y\n",
    "        ).set_title(f'{score_function} {y}')\n",
    "\n",
    "        g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "347.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
